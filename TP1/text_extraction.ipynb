{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ab08b5",
   "metadata": {},
   "source": [
    "Text Extraction\n",
    "From the following link, extract the text (title and abstract) of each scientific article, then save it in a text file named ùêÉ < ùíä >. ùê≠ùê±ùê≠, where 1 ‚â§ ùëñ ‚â§ ùêç, with ùêç being\n",
    "the number of scientific articles.\n",
    "https://link.springer.com/journal/12065/volumes-and-issues/18-5\n",
    "Text Preprocessing\n",
    "- Create a file T.txt containing all the tokens of the extracted scientific articles, using the Regex function from NLTK.\n",
    "- Create a file T_N.txt containing all the normalized tokens of the extracted scientific articles, using the Snowball stemming function from NLTK. Tokens must\n",
    "be converted to lowercase first.\n",
    "- Create a file V.txt containing the vocabulary (set of unique words) of all the extracted scientific articles, using the Regex function from NLTK.\n",
    "- Create a file V_N.txt containing the vocabulary (set of unique normalized words) of all the extracted scientific articles, using the Regex function from NLTK.\n",
    "- Create a file S.txt containing all the sentences of the extracted scientific articles, using the Regex function from NLTK.\n",
    "Data Visualization\n",
    "Using the WordCloud library, visualize the content of the files T.txt and T_N.txt.\n",
    "Edit Distance\n",
    "Implement the algorithm that computes the minimum edit distance (or Levenshtein distance) between two strings, as covered in class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e5858",
   "metadata": {},
   "source": [
    "# Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02789cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12f6eb",
   "metadata": {},
   "source": [
    "# Extracting the title and abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56248305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_articles():\n",
    "    html_text = requests.get('https://link.springer.com/journal/12065/volumes-and-issues/18-5').text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    # Find all article links (adjust selector as needed)\n",
    "    links = soup.find_all('a', href=lambda x: x and x.startswith('/article/'))\n",
    "    i=0\n",
    "    for link in links:\n",
    "        i= i+1\n",
    "        href = link.get('href')\n",
    "        title = link.get_text(strip=True)\n",
    "\n",
    "        # retriving the abstract\n",
    "        article_url = f'https://link.springer.com{href}'\n",
    "        article_html = requests.get(article_url).text\n",
    "        article_soup = BeautifulSoup(article_html, 'html.parser')\n",
    "\n",
    "        # Using the specific class \n",
    "        abstract_div = article_soup.find('div', {'class': 'c-article-section__content'})\n",
    "        if abstract_div:\n",
    "            abstract = abstract_div.find('p').get_text(strip=True)\n",
    "        else:\n",
    "            abstract = \"Abstract not found\"\n",
    "\n",
    "        # saving title and abstract to a text file\n",
    "        with open(f'D{i}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(f'Title: {title}\\n\\n')\n",
    "            f.write(f'Abstract: {abstract}\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        extract_all_articles()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
