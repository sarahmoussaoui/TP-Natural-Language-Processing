SELFVarBL: a Stacking-based Ensemble Learning Framework with Variable numbers of Base-Learners

Stacking is one of the most popular ensemble learning technique that combines the predictions of multiple base-learners (BLs) using a meta-learner to improve the overall performance of the model. To build a stacking-based ensemble, most of the existing works select top-k-performing models as the base-learners without considering the computational resource requirement of the system. This work presents a Stacking-based Ensemble Learning Framework with a Variable number of Base-Learners (SELFVarBL), which is capable of finding the trade-off between computational resource requirements and system performance by automatically selecting a suitable number and combinations of base-learners. Additionally, the proposed framework does not require excessive computational resources to achieve the optimal solution. The proposed framework is validated using three case studies with diversify datasets: the agriculture domain, the audio data analysis domain, and the self-generated synthetic datasets. The experimental results show the effectiveness of the proposed framework.
